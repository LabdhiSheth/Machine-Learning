{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn import datasets,metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[range(0,150,2),:]\n",
    "y_train = y[range(0,150,2)]\n",
    "\n",
    "X_test = X[range(1,150,2),:]\n",
    "y_test = y[range(1,150,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :- \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2\n",
      " 1]\n",
      "\n",
      "Accuracy score :-\n",
      " 0.9466666666666667\n",
      "\n",
      "Classification report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.89      0.96      0.92        25\n",
      "           2       0.96      0.88      0.92        25\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.95      0.95      0.95        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n",
      "\n",
      "Confusion Matrix :-\n",
      " [[25  0  0]\n",
      " [ 0 24  1]\n",
      " [ 0  3 22]]\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion = 'entropy').fit(X_train,y_train)\n",
    "\n",
    "prediction = tree.predict(X_test)\n",
    "print(\"Prediction :- \\n\",prediction)\n",
    "\n",
    "print(\"\\nAccuracy score :-\\n\",metrics.accuracy_score(y_test, prediction, normalize= True))\n",
    "\n",
    "print(\"\\nClassification report :- \\n\",metrics.classification_report(y_test, prediction))\n",
    "\n",
    "print(\"\\nConfusion Matrix :-\\n\",metrics.confusion_matrix(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :- \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2\n",
      " 1]\n",
      "\n",
      "Accuracy score :-\n",
      " 0.9466666666666667\n",
      "\n",
      "Classification report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.89      0.96      0.92        25\n",
      "           2       0.96      0.88      0.92        25\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.95      0.95      0.95        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n",
      "\n",
      "Confusion Matrix :-\n",
      " [[25  0  0]\n",
      " [ 0 24  1]\n",
      " [ 0  3 22]]\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\").fit(X_train, y_train)\n",
    "\n",
    "prediction = clf_gini.predict(X_test)\n",
    "print(\"Prediction :- \\n\",prediction)\n",
    "\n",
    "print(\"\\nAccuracy score :-\\n\",metrics.accuracy_score(y_test, prediction, normalize= True))\n",
    "\n",
    "print(\"\\nClassification report :- \\n\",metrics.classification_report(y_test, prediction))\n",
    "\n",
    "print(\"\\nConfusion Matrix :-\\n\",metrics.confusion_matrix(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gini.predict([[0,10, 3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical dataset - zoo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hair        float64\n",
      "feathers    float64\n",
      "eggs        float64\n",
      "milk        float64\n",
      "airbone     float64\n",
      "aquatic     float64\n",
      "predator    float64\n",
      "toothed     float64\n",
      "backbone    float64\n",
      "breathes    float64\n",
      "venomous    float64\n",
      "fins        float64\n",
      "legs        float64\n",
      "tail        float64\n",
      "domestic    float64\n",
      "catsize     float64\n",
      "class        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('zoo.csv',names=['animal_name','hair','feathers','eggs','milk',    \n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',    \n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])  \n",
    "\n",
    "dataset=dataset.drop('animal_name',axis=1)\n",
    "dataset['hair'] = pd.to_numeric(dataset['hair'], errors='coerce')\n",
    "dataset['feathers'] = pd.to_numeric(dataset['feathers'], errors='coerce')\n",
    "dataset['eggs'] = pd.to_numeric(dataset['eggs'], errors='coerce')\n",
    "dataset['milk'] = pd.to_numeric(dataset['milk'], errors='coerce')\n",
    "dataset['airbone'] = pd.to_numeric(dataset['airbone'], errors='coerce')\n",
    "dataset['aquatic'] = pd.to_numeric(dataset['aquatic'], errors='coerce')\n",
    "dataset['predator'] = pd.to_numeric(dataset['predator'], errors='coerce')\n",
    "dataset['toothed'] = pd.to_numeric(dataset['toothed'], errors='coerce')\n",
    "dataset['backbone'] = pd.to_numeric(dataset['backbone'], errors='coerce')\n",
    "dataset['breathes'] = pd.to_numeric(dataset['breathes'], errors='coerce')\n",
    "dataset['venomous'] = pd.to_numeric(dataset['venomous'], errors='coerce')\n",
    "dataset['fins'] = pd.to_numeric(dataset['fins'], errors='coerce')\n",
    "dataset['legs'] = pd.to_numeric(dataset['legs'], errors='coerce')\n",
    "dataset['tail'] = pd.to_numeric(dataset['tail'], errors='coerce')\n",
    "dataset['domestic'] = pd.to_numeric(dataset['domestic'], errors='coerce')\n",
    "dataset['catsize'] = pd.to_numeric(dataset['catsize'], errors='coerce')\n",
    "dataset = dataset.replace(np.nan, 0, regex=True)\n",
    "print (dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(dataset.iloc[:,:-1])\n",
    "y = pd.DataFrame(dataset.iloc[:,-1])\n",
    "\n",
    "X_train = X[:80]\n",
    "y_train = y[:80]\n",
    "\n",
    "X_test = X[80:]\n",
    "y_test = y[80:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :- \n",
      " ['2' '3' 'class_type' '4' '2' '1' '7' '4' '2' '6' '5' '6' '5' '4' '1' '1'\n",
      " '2' '1' '6' '1' 'class_type' '2']\n",
      "\n",
      "Accuracy score :-\n",
      " 0.8181818181818182\n",
      "\n",
      "Classification report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      0.33      0.50         3\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       0.50      1.00      0.67         1\n",
      "           6       0.67      1.00      0.80         2\n",
      "           7       1.00      0.33      0.50         3\n",
      "  class_type       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.82        22\n",
      "   macro avg       0.77      0.71      0.68        22\n",
      "weighted avg       0.95      0.82      0.83        22\n",
      "\n",
      "\n",
      "Confusion Matrix :-\n",
      " [[5 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 1 0 0]\n",
      " [0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 2 0 0]\n",
      " [0 0 0 0 0 0 1 2]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\labdh\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion = 'entropy').fit(X_train,y_train)\n",
    "\n",
    "prediction = tree.predict(X_test)\n",
    "print(\"Prediction :- \\n\",prediction)\n",
    "\n",
    "print(\"\\nAccuracy score :-\\n\",metrics.accuracy_score(y_test, prediction, normalize= True))\n",
    "\n",
    "print(\"\\nClassification report :- \\n\",metrics.classification_report(y_test, prediction))\n",
    "\n",
    "print(\"\\nConfusion Matrix :-\\n\",metrics.confusion_matrix(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :- \n",
      " ['2' '3' '6' '4' '2' '1' '7' '4' '2' '6' '5' '5' '5' '4' '1' '1' '2' '1'\n",
      " '6' '1' '6' '2']\n",
      "\n",
      "Accuracy score :-\n",
      " 0.8181818181818182\n",
      "\n",
      "Classification report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      0.33      0.50         3\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       0.33      1.00      0.50         1\n",
      "           6       0.50      1.00      0.67         2\n",
      "           7       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.82        22\n",
      "   macro avg       0.83      0.81      0.74        22\n",
      "weighted avg       0.92      0.82      0.81        22\n",
      "\n",
      "\n",
      "Confusion Matrix :-\n",
      " [[5 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0]\n",
      " [0 0 1 0 2 0 0]\n",
      " [0 0 0 3 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 2 1]]\n"
     ]
    }
   ],
   "source": [
    "tree_gini = DecisionTreeClassifier(criterion = 'gini').fit(X_train,y_train)\n",
    "\n",
    "prediction = tree_gini.predict(X_test)\n",
    "print(\"Prediction :- \\n\",prediction)\n",
    "\n",
    "print(\"\\nAccuracy score :-\\n\",metrics.accuracy_score(y_test, prediction, normalize= True))\n",
    "\n",
    "print(\"\\nClassification report :- \\n\",metrics.classification_report(y_test, prediction))\n",
    "\n",
    "print(\"\\nConfusion Matrix :-\\n\",metrics.confusion_matrix(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
